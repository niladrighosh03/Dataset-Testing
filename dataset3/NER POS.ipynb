{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8ebf67-0183-4759-8df2-432f6ad6e245",
   "metadata": {},
   "source": [
    "## POS and NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663ff01e-249c-4b43-af9c-5f318f909ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "474cecc6-3f60-460c-bf98-fffd24ee4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6361d775-5664-42a8-87fe-e74856cf974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part-of-Speech Tags:\n",
      "Barack -> PROPN (NNP)\n",
      "Obama -> PROPN (NNP)\n",
      "was -> AUX (VBD)\n",
      "born -> VERB (VBN)\n",
      "in -> ADP (IN)\n",
      "Hawaii -> PROPN (NNP)\n",
      "and -> CCONJ (CC)\n",
      "served -> VERB (VBD)\n",
      "as -> ADP (IN)\n",
      "the -> DET (DT)\n",
      "44th -> ADJ (JJ)\n",
      "President -> PROPN (NNP)\n",
      "of -> ADP (IN)\n",
      "the -> DET (DT)\n",
      "United -> PROPN (NNP)\n",
      "States -> PROPN (NNP)\n",
      ". -> PUNCT (.)\n",
      "\n",
      "Named Entities:\n",
      "Barack Obama -> PERSON\n",
      "Hawaii -> GPE\n",
      "44th -> ORDINAL\n",
      "the United States -> GPE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your input sentence\n",
    "sentence = \"Barack Obama was born in Hawaii and served as the 44th President of the United States.\"\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# POS tagging\n",
    "print(\"Part-of-Speech Tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.pos_} ({token.tag_})\")\n",
    "\n",
    "# Named Entity Recognition\n",
    "print(\"\\nNamed Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db531f-b44a-46b3-8b91-7873a6e07201",
   "metadata": {},
   "source": [
    "## Topic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f00c7a3a-514f-4982-aa0b-a90e802d9652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Segment 1 ---\n",
      "\n",
      "Artificial Intelligence is transforming industries. Itâ€™s used in healthcare for diagnostics and treatment suggestions.\n",
      "Itâ€™s also being used in agriculture to optimize crop yield and monitor soil conditions.\n",
      "\n",
      "--- Segment 2 ---\n",
      "\n",
      "\n",
      "Meanwhile, climate change is becoming more severe. Governments are pushing green policies to curb emissions.\n",
      "Technological innovations are focusing on renewable energy and carbon capture solutions.\n",
      "\n",
      "In sports, the Olympics will feature new events like skateboarding and surfing.\n",
      "Athletes are using AI-based tools to analyze performance and prevent injuries.\n",
      "\n",
      "Space exploration is also advancing. Private companies are launching missions to the Moon and Mars.\n",
      "Thereâ€™s a renewed interest in deep space communication and building space habitats.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "\n",
    "# ðŸ”§ Tell NLTK where to find your offline corpora\n",
    "nltk.data.path.insert(0, \"/home/rohank__iitp/nltk_data\")\n",
    "\n",
    "# ðŸ§¾ Sample long text (needs enough content for topic segmentation)\n",
    "text = \"\"\"\n",
    "Artificial Intelligence is transforming industries. Itâ€™s used in healthcare for diagnostics and treatment suggestions.\n",
    "Itâ€™s also being used in agriculture to optimize crop yield and monitor soil conditions.\n",
    "\n",
    "Meanwhile, climate change is becoming more severe. Governments are pushing green policies to curb emissions.\n",
    "Technological innovations are focusing on renewable energy and carbon capture solutions.\n",
    "\n",
    "In sports, the Olympics will feature new events like skateboarding and surfing.\n",
    "Athletes are using AI-based tools to analyze performance and prevent injuries.\n",
    "\n",
    "Space exploration is also advancing. Private companies are launching missions to the Moon and Mars.\n",
    "Thereâ€™s a renewed interest in deep space communication and building space habitats.\n",
    "\"\"\"\n",
    "\n",
    "# text = \"\"\"\n",
    "# I am looking for a comprehensive policy for my Tesla car with roadside assistance.\n",
    "\n",
    "# This policy should also cover long-distance driving and emergency towing.\n",
    "\n",
    "# Electric vehicle insurance should include roadside assistance and battery failure response.\n",
    "# \"\"\"\n",
    "\n",
    "# ðŸ§  Create the tokenizer â€” with relaxed settings to handle short input\n",
    "tt = TextTilingTokenizer(w=20, k=5, smoothing_width=1)\n",
    "\n",
    "# ðŸ” Tokenize into topic segments\n",
    "segments = tt.tokenize(text)\n",
    "\n",
    "# ðŸ–¨ï¸ Print each segment\n",
    "for i, segment in enumerate(segments, 1):\n",
    "    print(f\"\\n--- Segment {i} ---\\n{segment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08814f-c900-4367-94af-4a1badb7e0d7",
   "metadata": {},
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d946501-c431-42e6-bbc3-cb48da81e10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Language probabilities: [en:0.999997542940732]\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, detect_langs\n",
    "text=\"Hi, I am a human.\"\n",
    "text = \"Hi, I am a human. I live on Earth and speak English fluently.\"\n",
    "text=\"Okay, that sounds pretty good. What would the premium be for the comprehensive policy?\"\n",
    "# text = \"Ceci est un exemple de texte en franÃ§ais.\"\n",
    "print(\"Detected language:\", detect(text))        # e.g., 'fr'\n",
    "print(\"Language probabilities:\", detect_langs(text))  # e.g., [fr:0.9999969999949514]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da81c13-9f4b-460f-aa2b-2f786a73b907",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b240122c-b98d-4ff1-8366-1f18057ca8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumps (ROOT)\n",
      "  fox (nsubj)\n",
      "    The (det)\n",
      "    quick (amod)\n",
      "    brown (amod)\n",
      "  over (prep)\n",
      "    dog (pobj)\n",
      "      the (det)\n",
      "      lazy (amod)\n",
      "  . (punct)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "def print_dependencies(token, level=0):\n",
    "    print(\"  \" * level + f\"{token.text} ({token.dep_})\")\n",
    "    for child in token.children:\n",
    "        print_dependencies(child, level + 1)\n",
    "\n",
    "# Start from the root (main verb)\n",
    "for token in doc:\n",
    "    if token.head == token:  # root\n",
    "        print_dependencies(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80c7d4d9-9e3c-430b-a5f7-cd6328d39bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        -> det        -> fox\n",
      "quick      -> amod       -> fox\n",
      "brown      -> amod       -> fox\n",
      "fox        -> nsubj      -> jumps\n",
      "jumps      -> ROOT       -> jumps\n",
      "over       -> prep       -> jumps\n",
      "the        -> det        -> dog\n",
      "lazy       -> amod       -> dog\n",
      "dog        -> pobj       -> over\n",
      ".          -> punct      -> jumps\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/rohank__iitp/.conda/envs/rohan12/lib/python3.12/site-packages/IPython/core/display.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken.text\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken.dep_\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken.head.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Visualize in browser\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m displacy.serve(doc, style=\u001b[33m\"\u001b[39m\u001b[33mdep\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/spacy/displacy/__init__.py:107\u001b[39m, in \u001b[36mserve\u001b[39m\u001b[34m(docs, style, page, minify, options, manual, port, host, auto_select_port)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_in_jupyter():\n\u001b[32m    106\u001b[39m     warnings.warn(Warnings.W011)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m render(docs, style=style, page=page, minify=minify, options=options, manual=manual)\n\u001b[32m    108\u001b[39m httpd = simple_server.make_server(host, port, app)\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUsing the \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m visualizer\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/rohank__iitp/.conda/envs/rohan12/lib/python3.12/site-packages/IPython/core/display.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "# Print dependency relationships\n",
    "for token in doc:\n",
    "    print(f\"{token.text:10} -> {token.dep_:10} -> {token.head.text}\")\n",
    "\n",
    "# Visualize in browser\n",
    "displacy.serve(doc, style=\"dep\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bb1a6-8ef1-488a-887d-3bd6575c10d6",
   "metadata": {},
   "source": [
    "## Relation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4150acb-8379-4e33-b964-f6da4a57a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: ('I', 'buy', 'Classic')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_SVO(doc):\n",
    "    \"\"\"\n",
    "    Yield (subject, verb, object) triples from a spaCy Doc.\n",
    "    \"\"\"\n",
    "    for token in doc:\n",
    "        # skip nonâ€‘verbs\n",
    "        if token.pos_ != \"VERB\":\n",
    "            continue\n",
    "\n",
    "        # subjects: nominal (nsubj) or passive (nsubjpass)\n",
    "        subjects = [w for w in token.lefts if w.dep_ in (\"nsubj\", \"nsubjpass\")]\n",
    "        if not subjects:\n",
    "            continue  # no subject, skip\n",
    "\n",
    "        # objects can be:\n",
    "        #   â€¢ direct object  (dobj)\n",
    "        #   â€¢ object of prep (pobj) reached via a preposition\n",
    "        #   â€¢ attribute or complement (attr) e.g. \"is president\"\n",
    "        objects = [w for w in token.rights if w.dep_ == \"dobj\"]\n",
    "\n",
    "        # look for prep â†’ pobj\n",
    "        for prep in (w for w in token.rights if w.dep_ == \"prep\"):\n",
    "            objects.extend([w for w in prep.rights if w.dep_ == \"pobj\"])\n",
    "\n",
    "        # attribute/complement (\"is president\")\n",
    "        objects.extend([w for w in token.rights if w.dep_ == \"attr\"])\n",
    "\n",
    "        if subjects and objects:\n",
    "            for s in subjects:\n",
    "                for o in objects:\n",
    "                    yield (s.text, token.lemma_, o.text)\n",
    "\n",
    "# text = \"Barack Obama was born in Hawaii and served as the president of the United States.\"\n",
    "text=\"I am looking for a comprehensive policy for my Tesla car with roadside assistance.\"\n",
    "\n",
    "# text=\"Okay, that sounds good. What about roadside assistance?\"\n",
    "text=\"Hi, I am interested in getting motor insurance for my bike. I just bought a new 2024 Royal Enfield Classic 350.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for triple in get_SVO(doc):\n",
    "    print(\"Relation:\", triple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006108c8-686d-4e36-b27a-43fb3053d252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
