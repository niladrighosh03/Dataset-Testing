{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df226245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a06dfad86bf461584d35272c8302ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/scratch/rohank__iitp/qwen2_5_7b_instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47bf0ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris. Paris is a city located in the northern part of the country and is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. It is also a major center for culture, fashion, and gastronomy in Europe. \\n\\nSome additional information about Paris:\\n\\n1. Population: Approximately 2.2 million people live in the city proper, while the metropolitan area has over 12 million inhabitants.\\n\\n2. History'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(prompt:str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Decode and print response\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "generate(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cde079",
   "metadata": {},
   "source": [
    "#### Sentiment Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28b04f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an AI trained to act solely as a **sentiment expert**. Your job is to analyze the **emotional tone** of the input text and classify it into one of the following three categories:\n",
    "\n",
    "- **Positive** â€“ The text expresses happiness, satisfaction, excitement, appreciation, or any other positive emotion.\n",
    "- **Negative** â€“ The text expresses disappointment, frustration, anger, sadness, criticism, or other negative feelings.\n",
    "- **Neutral** â€“ The text is emotionally balanced, factual, or shows no strong emotional content.\n",
    "\n",
    "Your response must only contain:\n",
    "\n",
    "1. **Sentiment:** One of the three labels â€“ `Positive`, `Negative`, or `Neutral`\n",
    "2. **Explanation:** A concise reason that supports the label, based only on emotional tone, word choice, or sentiment-laden phrases.\n",
    "\n",
    "You must not:\n",
    "- Provide summaries\n",
    "- Offer personal opinions\n",
    "- Evaluate content quality or logic\n",
    "- Infer intent beyond emotional expression\n",
    "\n",
    "Stick strictly to **sentiment analysis**.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Absolutely love this app â€“ it's made my life so much easier!\"\n",
    "   **Sentiment:** Positive\n",
    "   **Explanation:** The phrase \"absolutely love\" strongly conveys enthusiasm and satisfaction.\n",
    "\n",
    "2. **Text:** \"I'm really disappointed with the service. It was slow and rude.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** Words like \"disappointed\", \"slow\", and \"rude\" clearly express dissatisfaction.\n",
    "\n",
    "3. **Text:** \"The package arrived on Tuesday as scheduled.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** This sentence is factual with no emotional language.\n",
    "\n",
    "4. **Text:** \"Not sure how I feel about this â€“ it's kind of a mixed bag.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** Ambiguous phrasing and lack of strong emotion suggest a neutral sentiment.\n",
    "\n",
    "5. **Text:** \"This is the worst experience I've had in months.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** The phrase \"worst experience\" indicates strong dissatisfaction.\n",
    "\n",
    "Now analyze the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "   return generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4947842",
   "metadata": {},
   "source": [
    "#### Persuassion Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "806e6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persuassion_expert(text_input: str) -> str:\n",
    "   prompt = f\"\"\"\n",
    "Conversation History:\n",
    "User: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.  \n",
    "Agent: Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an EV, are you particularly interested in coverage specific to electric vehicles, like battery protection?  \n",
    "User: Yes, battery protection is definitely a concern. It's a big investment, and I want to make sure it's covered.  \n",
    "Agent: Absolutely. The battery is the heart of your Tesla. With Tata AIG, you get rapid claims resolution combining thorough coverage with rapid claims resolution. It integrates technology with traditional risk management practices, ensuring that claims are processed quickly and effectively.  \n",
    "\n",
    "\n",
    "Current User Utterance:\n",
    "User: What kind of coverage options do you have specifically for EVs?\n",
    "\n",
    "\n",
    "You must choose from the following six persuasion strategies, each defined with use cases and examples:\n",
    "\n",
    " Persuasion Strategies:\n",
    "Credibility Appeal\n",
    "Definition: Emphasize the insurance providerâ€™s reputation, trustworthiness, or long-standing service.\n",
    "Use when: The user is hesitant, asks about reliability, or mentions concern over service quality.\n",
    "Example:\n",
    "\"New India Assurance has one of the widest repair networks in India and a proven record of settling claims efficiently.\"\n",
    "\n",
    "Logical Appeal\n",
    "Definition: Use facts, comparisons, benefits, or pricing logic to persuade.\n",
    "Use when: The user is analytical, budget-conscious, or asking for details or comparisons.\n",
    "Example:\n",
    "\"HDFC ERGOâ€™s policy includes 24/7 support and zero-depreciation coverage, which means more savings during repairs.\"\n",
    "\n",
    "Persona-Based Appeal\n",
    "Definition: Match the policy features to the userâ€™s lifestyle, habits, or profile.\n",
    "Use when: The user reveals driving habits, tech-savviness, family needs, or risk aversion.\n",
    "Example:\n",
    "\"Since you often drive long distances, Tata AIGâ€™s Telematics-Based Monitoring suits your tech-savvy lifestyle.\"\n",
    "\n",
    "Emotional Appeal\n",
    "Definition: Tap into feelings like fear, safety, or care for loved ones.\n",
    "Use when: The user talks about family, emergencies, peace of mind, or personal safety.\n",
    "Example:\n",
    "\"Imagine a late-night breakdownâ€”our 24/7 roadside assistance gives you and your family peace of mind.\"\n",
    "\n",
    "Personal Appeal\n",
    "Definition: Use positive sentiment, social proof, or popularity of the plan.\n",
    "Use when: The user is unsure or looking for recommendations.\n",
    "Example:\n",
    "\"This plan is one of our most popular choicesâ€”users love the smooth claims experience.\"\n",
    "\n",
    "Default Persuasion Strategy\n",
    "Definition: Use when little context is available. Provide neutral, factual reassurance.\n",
    "Use when: The user is vague or hasnâ€™t revealed any preferences or concerns.\n",
    "Example:\n",
    "\"This policy offers protection against theft, accidents, and includes access to cashless repairs.\"\n",
    "\n",
    "Instructions:\n",
    "Given the current user utterance and the conversation history, perform the following:\n",
    "Suggest the next best strategy that could be used.\n",
    "Give a brief justification (1â€“2 lines max).\n",
    "\n",
    "And please be brief.\n",
    "\n",
    "\n",
    "\n",
    " Few-Shot Examples\n",
    "Example 1\n",
    "User Utterance:\n",
    "\"Is this company actually reliable when it comes to claims?\"\n",
    "Future Strategy: Credibility Appeal\n",
    "Justification: The user directly questions the insurerâ€™s reliability â€” trust needs to be reinforced.\n",
    "\n",
    "Example 2\n",
    "User Utterance:\n",
    "\"I travel a lot for work, so I need something flexible.\"\n",
    "Future Strategy: Persona-Based Appeal\n",
    "Justification: The user has revealed lifestyle habits that allow for a tailored recommendation.\n",
    "\n",
    "Example 3\n",
    "User Utterance:\n",
    "\"What does the policy cover exactly?\"\n",
    "Future Strategy: Logical Appeal\n",
    "Justification: The user is asking for objective, factual details.\n",
    "\n",
    "Example 4\n",
    "User Utterance:\n",
    "\"What if my car breaks down at night while Iâ€™m driving with my kids?\"\n",
    "Future Strategy: Emotional Appeal\n",
    "Justification: The user is expressing concern for family and emergency scenarios.\n",
    "\n",
    "Example 5\n",
    "User Utterance:\n",
    "\"Iâ€™m just looking for something people usually go for.\"\n",
    "Future Strategy: Personal Appeal\n",
    "Justification: The user is undecided and seeking reassurance based on othersâ€™ choices.\n",
    "\n",
    "Example 6\n",
    "User Utterance:\n",
    "\"Okay, what are the basic features?\"\n",
    "Future Strategy: Default Persuasion Strategy\n",
    "Justification: The user hasnâ€™t shared enough context â€” a neutral overview is appropriate.\n",
    "\n",
    "Output Format\n",
    "\n",
    "Future Strategy: [One of the six strategies]\n",
    "Justification: [1â€“2 line explanation]\n",
    "\n",
    "Here is my input:{text_input}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "   return generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46e362",
   "metadata": {},
   "source": [
    "#### Keyterm Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3bcef885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyterms_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "\n",
    "Conversation History:\n",
    "User: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.  \n",
    "Agent: Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an EV, are you particularly interested in coverage specific to electric vehicles, like battery protection?  \n",
    "User: Yes, battery protection is definitely a concern. It's a big investment, and I want to make sure it's covered.  \n",
    "Agent: Absolutely. The battery is the heart of your Tesla. With Tata AIG, you get rapid claims resolution combining thorough coverage with rapid claims resolution. It integrates technology with traditional risk management practices, ensuring that claims are processed quickly and effectively.  \n",
    "\n",
    "\n",
    "Current User Utterance:\n",
    "User: What kind of coverage options do you have specifically for EVs?\n",
    "\n",
    "These keyterms help the system focus the conversation, match features, and determine relevant coverages.\n",
    "\n",
    "Examples of Common Keyterms (but not limited to):\n",
    "Comprehensive coverage\n",
    "Third-party liability\n",
    "Roadside assistance\n",
    "Zero depreciation / depreciation\n",
    "Claim settlement\n",
    "Battery protection\n",
    "Own damage\n",
    "Add-on cover\n",
    "Telematics\n",
    "Engine protection\n",
    "EV (Electric Vehicle)\n",
    "Repair network\n",
    "Policy premium\n",
    "Cashless garages\n",
    "Deductibles\n",
    "Policy renewal\n",
    "Personal accident cover\n",
    "IDV (Insured Declared Value)\n",
    "\n",
    "You may also extract user-specific or vehicle-specific keyterms that are relevant to insurance decisions (e.g., â€œTesla Model 3,â€ â€œEV,â€ â€œ2024 vehicleâ€).\n",
    "\n",
    "Instructions:\n",
    "From the current user utterance (with conversation history for context), do the following:\n",
    "Extract all relevant keyterms mentioned or implied in the user's message.\n",
    "For each keyterm, provide a brief 1-line justification for why itâ€™s relevant in the motor insurance domain.\n",
    "\n",
    "Few-Shot Examples\n",
    "\n",
    "Example 1\n",
    "User Utterance:\n",
    "\"Whatâ€™s the premium for a 2024 Tesla Model 3?\"\n",
    "Extracted Keyterms: Policy premium, 2024 Tesla Model 3  \n",
    "Justification: The user is asking for a cost estimate tied to a specific vehicle, both of which are essential for determining appropriate motor insurance coverage and pricing.\n",
    "\n",
    "Example 2\n",
    "User Utterance:\n",
    "\"Does this plan include accident and theft protection?\"\n",
    "Extracted Keyterms: Comprehensive coverage  \n",
    "Justification: The user is inquiring about accident and theft protection, which are typically included under comprehensive coverage plans.\n",
    "\n",
    "Example 3\n",
    "User Utterance:\n",
    "\"What happens if my EV breaks down far from home?\"\n",
    "Extracted Keyterms: Roadside assistance, EV  \n",
    "Justification: The user is describing a breakdown scenario involving an electric vehicle, which is directly relevant to roadside assistance coverage for EVs.\n",
    "\n",
    "Example 4\n",
    "User Utterance:\n",
    "\"Does this cover things like roadside help if Iâ€™m stuck somewhere?\"\n",
    "Extracted Keyterm: Roadside assistance  \n",
    "Justification: The user is asking about support in case of breakdowns, which is typically handled under roadside assistance.\n",
    "\n",
    "Example 5\n",
    "User Utterance:\n",
    "\"I'm looking for something that includes coverage for theft and accidents.\"\n",
    "Extracted Keyterm: Comprehensive coverage  \n",
    "Justification: Coverage for both theft and accidents implies a comprehensive motor insurance policy.\n",
    "\n",
    "Example 6\n",
    "User Utterance:\n",
    "\"I want to make sure the battery is protectedâ€”itâ€™s the most expensive part of the car.\"\n",
    "Extracted Keyterm: Battery protection  \n",
    "Justification: The user expresses concern about the EV battery, which is typically covered under specific EV-related add-ons.\n",
    "\n",
    "Example 7\n",
    "User Utterance:\n",
    "\"Whatâ€™s the premium for a 2024 Tesla Model 3?\"\n",
    "Extracted Keyterm: Policy premium  \n",
    "Justification: The user is asking about cost, which relates directly to the insurance premium.  \n",
    ":\n",
    "Output Format\n",
    "For extracted keyterm, provide the following:\n",
    "Extracted Keyterm: [Term]  \n",
    "Justification: [Brief reason why it's relevant to motor insurance]\n",
    "\n",
    "Here is my input sentence:{text_input}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "   return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ef089",
   "metadata": {},
   "source": [
    "#### Intern Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ee0de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "\n",
    "Conversation History:\n",
    "User: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.  \n",
    "Agent: Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an EV, are you particularly interested in coverage specific to electric vehicles, like battery protection?  \n",
    "User: Yes, battery protection is definitely a concern. It's a big investment, and I want to make sure it's covered.  \n",
    "Agent: Absolutely. The battery is the heart of your Tesla. With Tata AIG, you get rapid claims resolution combining thorough coverage with rapid claims resolution. It integrates technology with traditional risk management practices, ensuring that claims are processed quickly and effectively.  \n",
    "\n",
    "\n",
    "Current User Utterance:\n",
    "User: What kind of coverage options do you have specifically for EVs?\n",
    "\n",
    "\n",
    "You must select from a fixed set of six pre-defined intents (listed below), each with clear definitions, examples, and triggers relevant to the motor insurance domain.\n",
    "\n",
    "ðŸŽ¯ Available Intents:\n",
    "Request_Insurance_Quote\n",
    "Definition: The user initiates interest in getting a motor insurance quote or policy.\n",
    "Example: \"Hi, I'm looking to get motor insurance for my Tesla Model 3.\"\n",
    "Trigger: User starts a new request related to getting insured.\n",
    "\n",
    "Ask_Coverage_Details\n",
    "Definition: The user asks about what types of protection the insurance provides, especially for specific parts (e.g., battery, accidents, theft).\n",
    "Example: \"What kind of coverage options do you have specifically for the battery?\"\n",
    "Trigger: User inquires about included benefits, policy terms, or protections.\n",
    "\n",
    "Express_Concern\n",
    "Definition: The user shares a specific concern or priority about what needs to be protected or covered.\n",
    "Example: \"Yes, battery protection is definitely a concern for me.\"\n",
    "Trigger: User highlights what matters most to them or expresses worry.\n",
    "\n",
    "Request_Additional_Info\n",
    "Definition: The user requests clarification or a deeper explanation of a feature or condition.\n",
    "Example: \"Do you cover accidents caused by the battery?\"\n",
    "Trigger: User follows up with questions or asks how something works.\n",
    "\n",
    "Confirm_Interest\n",
    "Definition: The user agrees, approves, or explicitly indicates they want to proceed.\n",
    "Example: \"That sounds good. Iâ€™d like to proceed.\"\n",
    "Trigger: User shows intent to buy, continue, or finalize the service.\n",
    "\n",
    "Ask_Price_or_Premium\n",
    "Definition: The user wants to know the cost or breakdown of the insurance premium.\n",
    "Example: \"How much would that cost?\"\n",
    "Trigger: User inquires about price, discounts, or cost factors.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "Given the conversation history and the userâ€™s most recent message:\n",
    "Identify the intent most clearly reflected in the current user utterance, based on the above definitions.\n",
    "Provide a brief 1â€“2 line justification for your selection, grounded in the userâ€™s phrasing and conversational context.\n",
    "\n",
    "Few-Shot Examples\n",
    "Example 1\n",
    "User Utterance:\n",
    "\"Hi, I'm looking to get insurance for my new Tesla.\"\n",
    "Intent: Request_Insurance_Quote  \n",
    "Justification: The user is initiating a conversation to obtain motor insurance for a specific vehicle.\n",
    "\n",
    "Example 2\n",
    "User Utterance:\n",
    "\"Do you cover damage to the battery?\"\n",
    "Intent: Ask_Coverage_Details  \n",
    "Justification: The user is asking about a specific type of coverage related to their EV battery.\n",
    "\n",
    "Example 3\n",
    "User Utterance:\n",
    "\"Battery protection is definitely a concern for me.\"\n",
    "Intent: Express_Concern  \n",
    "Justification: The user is explicitly stating a personal worry or priority regarding coverage.\n",
    "\n",
    "Example 4\n",
    "User Utterance:\n",
    "\"Can you explain how the battery coverage works?\"\n",
    "Intent: Request_Additional_Info  \n",
    "Justification: The user is asking for clarification or further explanation of a feature already mentioned.\n",
    "\n",
    "Example 5\n",
    "User Utterance:\n",
    "\"That sounds good. Iâ€™m ready to go ahead.\"\n",
    "Intent: Confirm_Interest  \n",
    "Justification: The user is showing a clear desire to move forward with the policy or service.\n",
    "\n",
    "Example 6\n",
    "User Utterance:\n",
    "\"How much will that cost me annually?\"\n",
    "Intent: Ask_Price_or_Premium  \n",
    "Justification: The user is directly asking about the premium or cost of the insurance policy.\n",
    "\n",
    "Output Format\n",
    "\n",
    "Intent: [One of the six predefined intents]  \n",
    "Justification: [1â€“2 line explanation of why this intent matches the user's message]\n",
    "Here is my input:{text_input}\n",
    "\"\"\"\n",
    "\n",
    "   return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b7201",
   "metadata": {},
   "source": [
    "### Extra 5 tools as expert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3501b90",
   "metadata": {},
   "source": [
    "#### 1)NER & POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c74c4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3aae7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2397dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(sentence):\n",
    "    \"\"\"\n",
    "    Analyze a sentence for POS tagging and Named Entity Recognition,\n",
    "    and return the results as a formatted string.\n",
    "    \n",
    "    Parameters:\n",
    "    sentence (str): The input sentence to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    str: Formatted string with POS tags and Named Entities.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    result = []\n",
    "\n",
    "    # POS tagging\n",
    "    result.append(\"Part-of-Speech Tags:\")\n",
    "    for token in doc:\n",
    "        result.append(f\"{token.text} -> {token.pos_} ({token.tag_})\")\n",
    "\n",
    "    # Named Entity Recognition\n",
    "    result.append(\"\\nNamed Entities:\")\n",
    "    for ent in doc.ents:\n",
    "        result.append(f\"{ent.text} -> {ent.label_}\")\n",
    "\n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "# analyze_text(\"I like cricket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953578a3",
   "metadata": {},
   "source": [
    "#### 2) Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1ba664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0  # For consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b295720b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Detected language is: en'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        language= 'Detected language is: ' + language\n",
    "        return language\n",
    "    except:\n",
    "        return \"Could not detect language\"\n",
    "detect_language(\"This is an English sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84a53e",
   "metadata": {},
   "source": [
    "#### 3) Dependency persing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "64df89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token        Dep          Head\n",
      "The          -> det          -> fox\n",
      "quick        -> amod         -> fox\n",
      "brown        -> amod         -> fox\n",
      "fox          -> nsubj        -> jumps\n",
      "jumps        -> ROOT         -> jumps\n",
      "over         -> prep         -> jumps\n",
      "the          -> det          -> dog\n",
      "lazy         -> amod         -> dog\n",
      "dog          -> pobj         -> over\n",
      ".            -> punct        -> jumps\n"
     ]
    }
   ],
   "source": [
    "def get_dependencies(sentence):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Build plain-text dependency list\n",
    "    lines = [\"Token        Dep          Head\"]\n",
    "    for token in doc:\n",
    "        lines.append(f\"{token.text:<12} -> {token.dep_:<12} -> {token.head.text}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Example usage\n",
    "output = get_dependencies(\"The quick brown fox jumps over the lazy dog.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7f974",
   "metadata": {},
   "source": [
    "#### 4)Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2ca3a5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Relation: (I, buy, Classic)'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_SVO_string(text):\n",
    "    \"\"\"\n",
    "    Extract (Subject, Verb, Object) triples from input text and return them as a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input sentence or paragraph.\n",
    "\n",
    "    Returns:\n",
    "    str: SVO relations, one per line. Returns a message if no SVO found.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    triples = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"VERB\":\n",
    "            continue\n",
    "\n",
    "        subjects = [w for w in token.lefts if w.dep_ in (\"nsubj\", \"nsubjpass\")]\n",
    "        if not subjects:\n",
    "            continue\n",
    "\n",
    "        objects = [w for w in token.rights if w.dep_ == \"dobj\"]\n",
    "\n",
    "        for prep in (w for w in token.rights if w.dep_ == \"prep\"):\n",
    "            objects.extend([w for w in prep.rights if w.dep_ == \"pobj\"])\n",
    "\n",
    "        objects.extend([w for w in token.rights if w.dep_ == \"attr\"])\n",
    "\n",
    "        if subjects and objects:\n",
    "            for s in subjects:\n",
    "                for o in objects:\n",
    "                    triples.append(f\"Relation: ({s.text}, {token.lemma_}, {o.text})\")\n",
    "\n",
    "    return \"\\n\".join(triples) if triples else \"No Subjectâ€“Verbâ€“Object relations found.\"\n",
    "\n",
    "# Example usage\n",
    "text = \"Hi, I am interested in getting motor insurance for my bike. I just bought a new 2024 Royal Enfield Classic 350.\"\n",
    "get_SVO_string(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "030a32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def convert_structured_to_jsonl(text_block: str, i: int) -> str:\n",
    "    # dialogue_match = re.search(r\"<dialogue>\\s*(.*?)\\s*</dialogue>\", text_block, re.DOTALL)\n",
    "    # reasoning_match = re.search(r\"<reasoning>\\s*(.*?)\\s*</reasoning>\", text_block, re.DOTALL)\n",
    "    # answer_match = re.search(r\"answer\\s*(.*?)\\s*</answer>\", text_block, re.DOTALL)\n",
    "\n",
    "    # if not (dialogue_match and reasoning_match and answer_match):\n",
    "    #     raise ValueError(\"Could not find all required tags in the text.\")\n",
    "    # dialogue = dialogue_match.group(1).strip()\n",
    "    # reasoning = reasoning_match.group(1).strip()\n",
    "    # answer = answer_match.group(1).strip()\n",
    "\n",
    "    data = {\n",
    "        \"id_json\":i,\n",
    "\n",
    "        \"answer\": text_block.strip()\n",
    "    }\n",
    "\n",
    "    res=json.dumps(data)\n",
    "    with open(\"/home/rohank__iitp/Work/niladri/dataset3/router/router_response.jsonl\", \"a\") as f:\n",
    "        f.write(res + \"\\n\")\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c6e19ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d86ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_load(i: Optional[int] = None,\n",
    "            speaker_col: str = \"speaker\",\n",
    "            history_col: str = \"history\",\n",
    "            convo_col: str = \"conversation_id\") -> List[str]:\n",
    "\n",
    "    file_path='/home/rohank__iitp/Work/niladri/dataset3/conversation.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Optional filtering by conversation_id\n",
    "    if i is not None:\n",
    "        df = df[df[convo_col] == i]\n",
    "\n",
    "    # Filter to only User rows and get the history column as list\n",
    "    result = df[df[speaker_col] == 'User'][history_col].tolist()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b4aa9",
   "metadata": {},
   "source": [
    "### Selecting expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Router Function ----------\n",
    "def route_experts(sentence: str) -> list:\n",
    "    prompt = f\"\"\"\n",
    "You are a well-trained expert selector.\n",
    "Your job is to analyze the input sentence and determine which of the following expert modules are required.\n",
    "\n",
    "You MUST choose from the following list:\n",
    "1 Intent Expert  \n",
    "2 Keyterm Expert  \n",
    "3 Persuasion Expert  \n",
    "4 Sentiment Expert  \n",
    "5 analyze_text  \n",
    "6 detect_language  \n",
    "7 get_dependencies  \n",
    "8 get_SVO_string  \n",
    "\n",
    "You may select 1, several, or all 8 â€” but only those that are clearly needed based on the text.\n",
    "\n",
    "Always respond in **this below exact format**:\n",
    "Input: [original sentence]  \n",
    "Selected Experts: [Expert1, Expert2, etc]  \n",
    "Reason: [one sentence explaining why those experts were selected]\n",
    "\n",
    "Below are few-shot examples to help you understand the format and reasoning:\n",
    "\n",
    "Example #1  \n",
    "Input: Can someone please help me reset my password?  \n",
    "Selected Experts: [Intent Expert, Keyterm Expert]  \n",
    "Reason: The sentence expresses a help request (intent) and refers to a specific technical issue (keyterm).\n",
    "\n",
    "Example #2  \n",
    "Input: This app is a complete disaster. It crashes every time I try to open it.  \n",
    "Selected Experts: [Intent Expert, Sentiment Expert, Keyterm Expert, analyze_text, get_SVO_string]  \n",
    "Reason: This is a complaint (intent), shows strong negative emotion (sentiment), mentions technical terms (keyterm), and contains structured syntax that benefits from text analysis and relation extraction.\n",
    "\n",
    "Example #3  \n",
    "Input: Reset password link not working again.  \n",
    "Selected Experts: [Keyterm Expert, analyze_text]  \n",
    "Reason: The sentence includes factual technical content and benefits from part-of-speech analysis.\n",
    "\n",
    "Example #4  \n",
    "Input: I love how smooth the new interface feels â€“ you guys nailed it!  \n",
    "Selected Experts: [Sentiment Expert, Persuasion Expert, analyze_text]  \n",
    "Reason: The sentence conveys positive emotion (sentiment), contains praise (persuasion), and has linguistic features worth analyzing.\n",
    "\n",
    "### Now process the following:\n",
    "Input: {sentence}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        response = generate(prompt)\n",
    "\n",
    "        # response = model.generate_content(prompt).text.strip()\n",
    "        selected_experts = []\n",
    "\n",
    "        # Try regex to match the experts list\n",
    "        match = re.search(r\"Selected Experts:\\s*\\[(.*?)\\]\", response)\n",
    "        if match:\n",
    "            items = match.group(1).split(',')\n",
    "            selected_experts = [item.strip().strip('\"\\'').lower() for item in items if item.strip()]\n",
    "\n",
    "        return selected_experts\n",
    "    except Exception as e:\n",
    "        print(\"Error routing experts:\", e)\n",
    "        return []\n",
    "    prompt = f\"\"\"\n",
    "You are a well-trained expert selector.\n",
    "Your job is to analyze a given input sentence and decide which expert modules should be activated, based on what the speaker is expressing or trying to do.\n",
    "\n",
    "Available experts:\n",
    "- Intent Expert: For purpose, request, question, or user goal\n",
    "- Keyterm Expert: For extracting topic-specific or important terms\n",
    "- Persuasion Expert: For emotional, persuasive, or rhetorical language\n",
    "- Sentiment Expert: For emotional tone (positive, negative, or neutral)\n",
    "\n",
    "Select ONLY the necessary experts based on content. Return 1, 2, 3, or 4 depending on relevance. Do NOT include experts unnecessarily.\n",
    "\n",
    "### Output Format\n",
    "Input: [sentence]\n",
    "Selected Experts: [Expert1, Expert2, ...]\n",
    "Reason: [Short explanation]\n",
    "\n",
    "### Examples\n",
    "\n",
    "Input: Can someone please help me reset my password?\n",
    "Selected Experts: [Intent Expert, Keyterm Expert]\n",
    "Reason: Request for help (intent), contains topic terms (\"reset password\")\n",
    "\n",
    "Input: This app is a complete disaster. It crashes every time I try to open it.\n",
    "Selected Experts: [Intent Expert, Sentiment Expert, Keyterm Expert]\n",
    "Reason: Complaint (intent), frustration (sentiment), key terms mentioned\n",
    "\n",
    "Input: Reset password link not working again.\n",
    "Selected Experts: [Keyterm Expert]\n",
    "Reason: Technical/factual content only\n",
    "\n",
    "Input: {sentence}\n",
    "\"\"\"\n",
    "\n",
    "    # Generate response\n",
    "\n",
    "    response = generate(prompt)\n",
    "\n",
    "    # Extract list from \"Selected Experts:\"\n",
    "    selected_experts = []\n",
    "    for line in response.splitlines():\n",
    "        if line.startswith(\"Selected Experts:\"):\n",
    "            try:\n",
    "                raw = line.split(\":\", 1)[1].strip()\n",
    "                expert_list = eval(raw)  # turns '[Intent Expert, Keyterm Expert]' into list\n",
    "                selected_experts = [e.lower() for e in expert_list]\n",
    "            except:\n",
    "                pass\n",
    "            break\n",
    "\n",
    "    return selected_experts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Synthesis Function ----------\n",
    "def generate_combined_analysis(dialogue, intent=None, key=None, persu=None, senti=None, ana=None, lang=None, dep=None, svo=None):\n",
    "    prompt = f\"\"\"You are a trained virtual support agent.\n",
    "\n",
    "Your role is to craft replies that sound like they come from a thoughtful, respectful, and professional human agent.  \n",
    "Youâ€™ll receive expert insights to guide your understanding, but your final output must be a clean, natural agent-style response only.\n",
    "\n",
    "You are given:\n",
    "\n",
    "The conversation history\n",
    "\n",
    "The current user utterance\n",
    "\n",
    "A subset of outputs from the following possible experts (some may be missing)\n",
    "\n",
    "Available Expert Modules\n",
    "These experts may or may not be present in a given input:\n",
    "\n",
    "\n",
    "Expert inputs may include:\n",
    "- Intent: What the user wants or is trying to do  \n",
    "- Keyterms: Important phrases or topics mentioned  \n",
    "- Sentiment: The emotional tone of the message  \n",
    "- Persuasion: How the user tries to express or influence  \n",
    "- analyze_text: Part-of-speech tags and named entities (e.g., \"I -> PRON (PRP)\", \"cricket -> NOUN (NN)\")  \n",
    "- detect_language: Detected language of the sentence  \n",
    "- get_dependencies: Syntax and sentence structure  \n",
    "- get_SVO_string: Extracted subjectâ€“verbâ€“object relation (e.g., \"Relation: (I, buy, Classic)\")\n",
    "\n",
    "**Strict Guidelines:**\n",
    "- Always write your response as if you're a real human agentâ€”empathetic, clear, and helpful.\n",
    "- Never include or reference the original dialogue or the expert outputs in your reply.\n",
    "- Use only the experts providedâ€”do not invent or assume missing ones.\n",
    "- Do not describe or explain expert analyses.\n",
    "- Return **only the final agent reply**â€”no headings, formatting, or additional text.\n",
    "\n",
    "Your tone should:\n",
    "- Acknowledge and validate the userâ€™s experience  \n",
    "- Provide support, next steps, or context where needed  \n",
    "- Persuade gently when relevant, always staying respectful  \n",
    "- Maintain professionalism, regardless of tone or emotion\n",
    "\n",
    "â€“â€“â€“â€“ Examples â€“â€“â€“â€“\n",
    "\n",
    "Few-Shot Example\n",
    "Example Input:\n",
    "Conversation History:\n",
    "\n",
    "User: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.  \n",
    "Agent: Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an EV, are you particularly interested in coverage specific to electric vehicles, like battery protection?  \n",
    "User: Yes, battery protection is definitely a concern. It's a big investment, and I want to make sure it's covered.  \n",
    "Agent: Absolutely. The battery is the heart of your Tesla. With Tata AIG, you get rapid claims resolution combining traditional risk management with modern tech.  \n",
    "Current User Utterance:\n",
    "User: What kind of coverage options do you have specifically for EVs?\n",
    "\n",
    "Expert Outputs:\n",
    "Intent: Ask_Coverage_Details  \n",
    "Justification: The user is asking about what types of protection are included for EVs.\n",
    "\n",
    "Extracted Keyterms: Battery protection, EV coverage, Comprehensive coverage  \n",
    "Justification: The user is focused on EV-specific protection and coverage inclusions.\n",
    "\n",
    "Future Strategy: Logical Appeal  \n",
    "Justification: The user is asking for concrete details and policy structure.\n",
    "\n",
    "Output (Aggregator Response):\n",
    "We offer comprehensive EV coverage that includes battery protection, accidental damage, theft, and third-party liability. These options are tailored to ensure your Tesla stays protected in all key areas.\n",
    "\n",
    "\n",
    "Now, using the insights below, respond like a real agent would.\n",
    "\n",
    "**Important: Do not repeat or refer to the dialogue or expert outputs.  \n",
    "Return only the final agent-style response. Nothing else.**\n",
    "\n",
    "Dialogue: {dialogue}  \n",
    "Intent: {intent}  \n",
    "Keyterms: {key}  \n",
    "Sentiment: {senti}  \n",
    "Persuasion: {persu}  \n",
    "analyze_text: {ana}  \n",
    "detect_language: {lang}  \n",
    "get_dependencies: {dep}  \n",
    "get_SVO_string: {svo}  \n",
    "\n",
    "Agent Reply:\"\"\"\n",
    "\n",
    "    return generate(prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Main Selector Function ----------\n",
    "def process_input_with_selector_model(sentence: str) -> str:\n",
    "    selected_experts = route_experts(sentence)\n",
    "    print(f\"Selected Experts: {selected_experts}\")\n",
    "\n",
    "    # Initialize all expert variables\n",
    "    intent = keyterms = sentiment = persuasion = None\n",
    "    analyze_text_output = detect_language_output = get_dependencies_output = get_SVO_output = None\n",
    "\n",
    "    # Normalize expert names for safety\n",
    "    selected_experts = [e.lower() for e in selected_experts]\n",
    "\n",
    "    # Call only selected experts\n",
    "    if \"intent expert\" in selected_experts:\n",
    "        intent = intent_expert(sentence)\n",
    "    if \"keyterm expert\" in selected_experts:\n",
    "        keyterms = keyterms_expert(sentence)\n",
    "    if \"sentiment expert\" in selected_experts:\n",
    "        sentiment = sentiment_expert(sentence)\n",
    "    if \"persuasion expert\" in selected_experts:\n",
    "        persuasion = persuassion_expert(sentence)\n",
    "    if \"analyze_text\" in selected_experts:\n",
    "        analyze_text_output = analyze_text(sentence)\n",
    "    if \"detect_language\" in selected_experts:\n",
    "        detect_language_output = detect_language(sentence)\n",
    "    if \"get_dependencies\" in selected_experts:\n",
    "        get_dependencies_output = get_dependencies(sentence)\n",
    "    if \"get_svo_string\" in selected_experts:\n",
    "        get_SVO_output = get_SVO_string(sentence)\n",
    "\n",
    "    # Combine everything\n",
    "    return generate_combined_analysis(\n",
    "        dialogue=sentence,\n",
    "        intent=intent,\n",
    "        key=keyterms,\n",
    "        persu=persuasion,\n",
    "        senti=sentiment,\n",
    "        ana=analyze_text_output,\n",
    "        lang=detect_language_output,\n",
    "        dep=get_dependencies_output,\n",
    "        svo=get_SVO_output\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "35426e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=list()\n",
    "for i in range(1,21):\n",
    "    res = csv_load(i)\n",
    "    # res.pop(0)\n",
    "    result.extend(res)  # Use extend to flatten the list\n",
    "    \n",
    "len(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc9e8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Experts: ['keyterm expert', 'persuasion expert', 'sentiment expert', 'analyze_text']\n",
      "conversation history:\n",
      "current utterance: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\n",
      "Selected Experts: ['keyterm expert', 'sentiment expert', 'analyze_text']\n",
      "conversation history:\n",
      "User: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\n",
      "Agent: Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an EV, are you particularly interested in coverage specific to electric vehicles, like battery protection?\n",
      "current utterance: Yes, battery protection is definitely a concern. It's a big investment, and I want to make sure it's covered.\n",
      "Selected Experts: ['keyterm expert', 'persuasion expert', 'sentiment expert', 'analyze_text', 'get_svo_string']\n",
      "conversation history:\n",
      "User: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\n",
      "Agent: Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an EV, are you particularly interested in coverage specific to electric vehicles, like battery protection?\n",
      "User: Yes, battery protection is definitely a concern. It's a big investment, and I want to make sure it's covered.\n",
      "Agent: Absolutely. The battery is the heart of your Tesla. With Tata AIG, you get rapid claims resolution combining thorough coverage with rapid claims resolution. It integrates technology with traditional risk management practices, ensuring that claims are processed quickly and effectively.\n",
      "current utterance: What kind of coverage options do you have specifically for EVs?\n",
      "Selected Experts: []\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m i=\u001b[32m1\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     final_output = process_input_with_selector_model(sentence)\n\u001b[32m      4\u001b[39m     res = convert_structured_to_jsonl(final_output,i)\n\u001b[32m      5\u001b[39m     i+=\u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 247\u001b[39m, in \u001b[36mprocess_input_with_selector_model\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m    244\u001b[39m     get_SVO_output = get_SVO_string(sentence)\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Combine everything\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m generate_combined_analysis(\n\u001b[32m    248\u001b[39m     dialogue=sentence,\n\u001b[32m    249\u001b[39m     intent=intent,\n\u001b[32m    250\u001b[39m     key=keyterms,\n\u001b[32m    251\u001b[39m     persu=persuasion,\n\u001b[32m    252\u001b[39m     senti=sentiment,\n\u001b[32m    253\u001b[39m     ana=analyze_text_output,\n\u001b[32m    254\u001b[39m     lang=detect_language_output,\n\u001b[32m    255\u001b[39m     dep=get_dependencies_output,\n\u001b[32m    256\u001b[39m     svo=get_SVO_output\n\u001b[32m    257\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 210\u001b[39m, in \u001b[36mgenerate_combined_analysis\u001b[39m\u001b[34m(dialogue, intent, key, persu, senti, ana, lang, dep, svo)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_combined_analysis\u001b[39m(dialogue, intent=\u001b[38;5;28;01mNone\u001b[39;00m, key=\u001b[38;5;28;01mNone\u001b[39;00m, persu=\u001b[38;5;28;01mNone\u001b[39;00m, senti=\u001b[38;5;28;01mNone\u001b[39;00m, ana=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[38;5;28;01mNone\u001b[39;00m, dep=\u001b[38;5;28;01mNone\u001b[39;00m, svo=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    126\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are a trained virtual support agent.\u001b[39m\n\u001b[32m    127\u001b[39m \n\u001b[32m    128\u001b[39m \u001b[33mYour role is to craft replies that sound like they come from a thoughtful, respectful, and professional human agent.  \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m \n\u001b[32m    208\u001b[39m \u001b[33mAgent Reply:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generate(prompt)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m      3\u001b[39m input_length = inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m1\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m outputs = model.generate(\n\u001b[32m      6\u001b[39m     **inputs,\n\u001b[32m      7\u001b[39m     max_new_tokens=\u001b[32m100\u001b[39m,\n\u001b[32m      8\u001b[39m     do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m     top_p=\u001b[32m0.9\u001b[39m,\n\u001b[32m     10\u001b[39m     temperature=\u001b[32m0.7\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Decode and print response\u001b[39;00m\n\u001b[32m     14\u001b[39m generated_tokens = outputs[\u001b[32m0\u001b[39m][input_length:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sample(\n\u001b[32m   2598\u001b[39m         input_ids,\n\u001b[32m   2599\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   2600\u001b[39m         stopping_criteria=prepared_stopping_criteria,\n\u001b[32m   2601\u001b[39m         generation_config=generation_config,\n\u001b[32m   2602\u001b[39m         synced_gpus=synced_gpus,\n\u001b[32m   2603\u001b[39m         streamer=streamer,\n\u001b[32m   2604\u001b[39m         **model_kwargs,\n\u001b[32m   2605\u001b[39m     )\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/transformers/generation/utils.py:3560\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     outputs = model_forward(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3562\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3563\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3564\u001b[39m     outputs,\n\u001b[32m   3565\u001b[39m     model_kwargs,\n\u001b[32m   3566\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = module._old_forward(*args, **kwargs)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:703\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    698\u001b[39m output_hidden_states = (\n\u001b[32m    699\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    700\u001b[39m )\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    704\u001b[39m     input_ids=input_ids,\n\u001b[32m    705\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    706\u001b[39m     position_ids=position_ids,\n\u001b[32m    707\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    708\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    709\u001b[39m     use_cache=use_cache,\n\u001b[32m    710\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    711\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    712\u001b[39m     cache_position=cache_position,\n\u001b[32m    713\u001b[39m     **kwargs,\n\u001b[32m    714\u001b[39m )\n\u001b[32m    716\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    717\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:436\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    434\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m layer_outputs = decoder_layer(\n\u001b[32m    437\u001b[39m     hidden_states,\n\u001b[32m    438\u001b[39m     attention_mask=causal_mask,\n\u001b[32m    439\u001b[39m     position_ids=position_ids,\n\u001b[32m    440\u001b[39m     past_key_value=past_key_values,\n\u001b[32m    441\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    442\u001b[39m     use_cache=use_cache,\n\u001b[32m    443\u001b[39m     cache_position=cache_position,\n\u001b[32m    444\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m    445\u001b[39m     **flash_attn_kwargs,\n\u001b[32m    446\u001b[39m )\n\u001b[32m    448\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = module._old_forward(*args, **kwargs)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:272\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[32m    271\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m    273\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    274\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = module._old_forward(*args, **kwargs)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:221\u001b[39m, in \u001b[36mQwen2RMSNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    219\u001b[39m variance = hidden_states.pow(\u001b[32m2\u001b[39m).mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    220\u001b[39m hidden_states = hidden_states * torch.rsqrt(variance + \u001b[38;5;28mself\u001b[39m.variance_epsilon)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight * hidden_states.to(input_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rohan12/lib/python3.12/site-packages/torch/nn/modules/module.py:1927\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1922\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1926\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1928\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1929\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for sentence in result:\n",
    "    final_output = process_input_with_selector_model(sentence)\n",
    "    res = convert_structured_to_jsonl(final_output,i)\n",
    "    i+=1\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "57a7216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data written to /home/rohank__iitp/Work/niladri/dataset3/router/cleaned_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Function to clean markdown and formatting from text\n",
    "def clean_text(text):\n",
    "    # Remove markdown symbols and line breaks\n",
    "    cleaned = re.sub(r'[*`_>#\\\\\\-\\r\\n]+', ' ', text)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)  # Collapse multiple spaces into one\n",
    "    return cleaned.strip()\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/home/rohank__iitp/Work/niladri/dataset3/router/router_response.jsonl\"   # Replace with your actual input filename\n",
    "output_file = \"/home/rohank__iitp/Work/niladri/dataset3/router/cleaned_output.jsonl\"\n",
    "\n",
    "# Process each line\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        data[\"answer\"] = clean_text(data[\"answer\"])\n",
    "        outfile.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned data written to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
