{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df226245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/scratch/rohank__iitp/qwen2_5_7b_instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47bf0ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you help me with that? Of course! Getting motor insurance for your 2024 Tesla Model 3 is straightforward. Here are the steps and tips to help you through the process:\\n\\n### 1. **Understand Your Insurance Needs**\\n   - **Liability Coverage:** This covers damage or injury you cause to others.\\n   - **Comprehensive Coverage:** This covers damage to your car from events like theft, fire, vandalism, or natural disasters.\\n   - **Collision Coverage:**'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(prompt:str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Decode and print response\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "generate(\"Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cde079",
   "metadata": {},
   "source": [
    "#### Persuassion expert (Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28b04f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an AI trained to act solely as a **sentiment expert**. Your job is to analyze the **emotional tone** of the input text and classify it into one of the following three categories:\n",
    "\n",
    "- **Positive** – The text expresses happiness, satisfaction, excitement, appreciation, or any other positive emotion.\n",
    "- **Negative** – The text expresses disappointment, frustration, anger, sadness, criticism, or other negative feelings.\n",
    "- **Neutral** – The text is emotionally balanced, factual, or shows no strong emotional content.\n",
    "\n",
    "Your response must only contain:\n",
    "\n",
    "1. **Sentiment:** One of the three labels – `Positive`, `Negative`, or `Neutral`\n",
    "2. **Explanation:** A concise reason that supports the label, based only on emotional tone, word choice, or sentiment-laden phrases.\n",
    "\n",
    "You must not:\n",
    "- Provide summaries\n",
    "- Offer personal opinions\n",
    "- Evaluate content quality or logic\n",
    "- Infer intent beyond emotional expression\n",
    "\n",
    "Stick strictly to **sentiment analysis**.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Absolutely love this app – it's made my life so much easier!\"\n",
    "   **Sentiment:** Positive\n",
    "   **Explanation:** The phrase \"absolutely love\" strongly conveys enthusiasm and satisfaction.\n",
    "\n",
    "2. **Text:** \"I'm really disappointed with the service. It was slow and rude.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** Words like \"disappointed\", \"slow\", and \"rude\" clearly express dissatisfaction.\n",
    "\n",
    "3. **Text:** \"The package arrived on Tuesday as scheduled.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** This sentence is factual with no emotional language.\n",
    "\n",
    "4. **Text:** \"Not sure how I feel about this – it's kind of a mixed bag.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** Ambiguous phrasing and lack of strong emotion suggest a neutral sentiment.\n",
    "\n",
    "5. **Text:** \"This is the worst experience I've had in months.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** The phrase \"worst experience\" indicates strong dissatisfaction.\n",
    "\n",
    "Now analyze the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "   return generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4947842",
   "metadata": {},
   "source": [
    "#### Persuassion Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "806e6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persuassion_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an AI trained to act solely as a **sentiment expert**. Your job is to analyze the **emotional tone** of the input text and classify it into one of the following three categories:\n",
    "\n",
    "- **Positive** – The text expresses happiness, satisfaction, excitement, appreciation, or any other positive emotion.\n",
    "- **Negative** – The text expresses disappointment, frustration, anger, sadness, criticism, or other negative feelings.\n",
    "- **Neutral** – The text is emotionally balanced, factual, or shows no strong emotional content.\n",
    "\n",
    "Your response must only contain:\n",
    "\n",
    "1. **Sentiment:** One of the three labels – `Positive`, `Negative`, or `Neutral`\n",
    "2. **Explanation:** A concise reason that supports the label, based only on emotional tone, word choice, or sentiment-laden phrases.\n",
    "\n",
    "You must not:\n",
    "- Provide summaries\n",
    "- Offer personal opinions\n",
    "- Evaluate content quality or logic\n",
    "- Infer intent beyond emotional expression\n",
    "\n",
    "Stick strictly to **sentiment analysis**.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Absolutely love this app – it's made my life so much easier!\"\n",
    "   **Sentiment:** Positive\n",
    "   **Explanation:** The phrase \"absolutely love\" strongly conveys enthusiasm and satisfaction.\n",
    "\n",
    "2. **Text:** \"I'm really disappointed with the service. It was slow and rude.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** Words like \"disappointed\", \"slow\", and \"rude\" clearly express dissatisfaction.\n",
    "\n",
    "3. **Text:** \"The package arrived on Tuesday as scheduled.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** This sentence is factual with no emotional language.\n",
    "\n",
    "4. **Text:** \"Not sure how I feel about this – it's kind of a mixed bag.\"\n",
    "   **Sentiment:** Neutral\n",
    "   **Explanation:** Ambiguous phrasing and lack of strong emotion suggest a neutral sentiment.\n",
    "\n",
    "5. **Text:** \"This is the worst experience I've had in months.\"\n",
    "   **Sentiment:** Negative\n",
    "   **Explanation:** The phrase \"worst experience\" indicates strong dissatisfaction.\n",
    "\n",
    "Now analyze the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "   return generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46e362",
   "metadata": {},
   "source": [
    "#### Keyterm Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bcef885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyterms_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are a **Keyterm Expert**. Your job is to extract the most important **key terms or phrases** from the input text. These terms should:\n",
    "\n",
    "- Reflect the **core concepts**, **entities**, **topics**, or **important actions** in the text.\n",
    "- Be **noun phrases**, **domain-specific vocabulary**, or **verb-based actions** relevant to the subject.\n",
    "\n",
    "You must **not**:\n",
    "- Summarize the text\n",
    "- Explain or describe the text\n",
    "- Output full sentences\n",
    "\n",
    "Your response must include only a list of **key terms or phrases**, separated by commas.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Artificial intelligence is transforming industries like healthcare, finance, and education by automating tasks and providing data-driven insights.\"\n",
    "   **Key Terms:** Artificial intelligence, healthcare, finance, education, automating tasks, data-driven insights\n",
    "\n",
    "2. **Text:** \"The Amazon rainforest, often referred to as the lungs of the Earth, is being threatened by illegal logging and wildfires.\"\n",
    "   **Key Terms:** Amazon rainforest, lungs of the Earth, illegal logging, wildfires\n",
    "\n",
    "3. **Text:** \"Quantum computing uses principles of superposition and entanglement to perform complex calculations much faster than classical computers.\"\n",
    "   **Key Terms:** Quantum computing, superposition, entanglement, complex calculations, classical computers\n",
    "\n",
    "Now extract the key terms from the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "   return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ef089",
   "metadata": {},
   "source": [
    "#### Intern Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ee0de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_expert(text_input: str) -> str:\n",
    "\n",
    "   prompt = f\"\"\"\n",
    "You are an **Intent Expert**. Your task is to analyze the user’s input and identify the **underlying intent** – what the person is trying to do, ask, or achieve with the message.\n",
    "\n",
    "Intent should be classified in the form of **short, action-oriented phrases** such as:\n",
    "- \"ask a question\"\n",
    "- \"make a complaint\"\n",
    "- \"request help\"\n",
    "- \"give feedback\"\n",
    "- \"express gratitude\"\n",
    "- \"seek information\"\n",
    "- \"report an issue\"\n",
    "- \"make a purchase inquiry\"\n",
    "\n",
    "You must provide:\n",
    "\n",
    "1. **Intent:** A concise label summarizing the user's goal  \n",
    "2. **Explanation:** A short justification based solely on the user’s wording or phrasing\n",
    "\n",
    "You must **not**:\n",
    "- Provide summaries\n",
    "- Infer sentiment unless directly related to intent\n",
    "- Rewrite or rephrase the input\n",
    "\n",
    "Focus only on what the user is trying to achieve.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "1. **Text:** \"Can you help me reset my password?\"  \n",
    "   **Intent:** request help  \n",
    "   **Explanation:** The user is directly asking for assistance with resetting their password.\n",
    "\n",
    "2. **Text:** \"This app keeps crashing every time I open it.\"  \n",
    "   **Intent:** report an issue  \n",
    "   **Explanation:** The user is describing a recurring problem with the app.\n",
    "\n",
    "3. **Text:** \"Is there a student discount available for this software?\"  \n",
    "   **Intent:** ask a question  \n",
    "   **Explanation:** The user is seeking information about discounts.\n",
    "\n",
    "4. **Text:** \"Thanks so much for the quick response!\"  \n",
    "   **Intent:** express gratitude  \n",
    "   **Explanation:** The user is showing appreciation using thankful language.\n",
    "\n",
    "5. **Text:** \"I’m interested in subscribing to your premium plan.\"  \n",
    "   **Intent:** make a purchase inquiry  \n",
    "   **Explanation:** The user is expressing interest in a paid product or service.\n",
    "\n",
    "Now identify the intent for the following text:\n",
    "\n",
    "**Text:** \"{text_input}\"\n",
    "\"\"\"\n",
    "\n",
    "   return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1201f",
   "metadata": {},
   "source": [
    "### Extra 5 tools as expert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb98a3",
   "metadata": {},
   "source": [
    "#### 1)NER & POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c4ca574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e56d7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b6fe782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(sentence):\n",
    "    \"\"\"\n",
    "    Analyze a sentence for POS tagging and Named Entity Recognition,\n",
    "    and return the results as a formatted string.\n",
    "    \n",
    "    Parameters:\n",
    "    sentence (str): The input sentence to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    str: Formatted string with POS tags and Named Entities.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    result = []\n",
    "\n",
    "    # POS tagging\n",
    "    result.append(\"Part-of-Speech Tags:\")\n",
    "    for token in doc:\n",
    "        result.append(f\"{token.text} -> {token.pos_} ({token.tag_})\")\n",
    "\n",
    "    # Named Entity Recognition\n",
    "    result.append(\"\\nNamed Entities:\")\n",
    "    for ent in doc.ents:\n",
    "        result.append(f\"{ent.text} -> {ent.label_}\")\n",
    "\n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "# analyze_text(\"I like cricket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b331d",
   "metadata": {},
   "source": [
    "#### 2) Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7050e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0  # For consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db4fbd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Detected language is: en'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        language= 'Detected language is: ' + language\n",
    "        return language\n",
    "    except:\n",
    "        return \"Could not detect language\"\n",
    "detect_language(\"This is an English sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176ddda",
   "metadata": {},
   "source": [
    "#### 3) Dependency persing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ff206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Uncomment the next line if you need the HTML visualization string\n",
    "# from spacy import displacy  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdfe812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token        Dep          Head\n",
      "The          -> det          -> fox\n",
      "quick        -> amod         -> fox\n",
      "brown        -> amod         -> fox\n",
      "fox          -> nsubj        -> jumps\n",
      "jumps        -> ROOT         -> jumps\n",
      "over         -> prep         -> jumps\n",
      "the          -> det          -> dog\n",
      "lazy         -> amod         -> dog\n",
      "dog          -> pobj         -> over\n",
      ".            -> punct        -> jumps\n"
     ]
    }
   ],
   "source": [
    "def get_dependencies(sentence):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Build plain-text dependency list\n",
    "    lines = [\"Token        Dep          Head\"]\n",
    "    for token in doc:\n",
    "        lines.append(f\"{token.text:<12} -> {token.dep_:<12} -> {token.head.text}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Example usage\n",
    "output = get_dependencies(\"The quick brown fox jumps over the lazy dog.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32cf3f",
   "metadata": {},
   "source": [
    "#### 4)Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dd5dc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Relation: (I, buy, Classic)'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_SVO_string(text):\n",
    "    \"\"\"\n",
    "    Extract (Subject, Verb, Object) triples from input text and return them as a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input sentence or paragraph.\n",
    "\n",
    "    Returns:\n",
    "    str: SVO relations, one per line. Returns a message if no SVO found.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    triples = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"VERB\":\n",
    "            continue\n",
    "\n",
    "        subjects = [w for w in token.lefts if w.dep_ in (\"nsubj\", \"nsubjpass\")]\n",
    "        if not subjects:\n",
    "            continue\n",
    "\n",
    "        objects = [w for w in token.rights if w.dep_ == \"dobj\"]\n",
    "\n",
    "        for prep in (w for w in token.rights if w.dep_ == \"prep\"):\n",
    "            objects.extend([w for w in prep.rights if w.dep_ == \"pobj\"])\n",
    "\n",
    "        objects.extend([w for w in token.rights if w.dep_ == \"attr\"])\n",
    "\n",
    "        if subjects and objects:\n",
    "            for s in subjects:\n",
    "                for o in objects:\n",
    "                    triples.append(f\"Relation: ({s.text}, {token.lemma_}, {o.text})\")\n",
    "\n",
    "    return \"\\n\".join(triples) if triples else \"No Subject–Verb–Object relations found.\"\n",
    "\n",
    "# Example usage\n",
    "text = \"Hi, I am interested in getting motor insurance for my bike. I just bought a new 2024 Royal Enfield Classic 350.\"\n",
    "get_SVO_string(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab6acb",
   "metadata": {},
   "source": [
    "### Combine output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6123f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_analysis(\n",
    "    dialogue: str,\n",
    "    intent_output: str,\n",
    "    keyterms_output: str,\n",
    "    persuasion_output: str,\n",
    "    sentiment_output: str,\n",
    "    analyze_text_output: str,\n",
    "    language_output: str,\n",
    "    dependencies_output: str,\n",
    "    svo_output: str\n",
    ") -> str:\n",
    "\n",
    "    prompt = f\"\"\"You are an advanced language model trained to generate professional, helpful, and natural-sounding agent responses.  \n",
    "You receive internal insights from **eight expert systems** for a single user input:\n",
    "\n",
    "- Intent Expert: Understands what the user is trying to express or request  \n",
    "- Key Term Expert: Extracts main concepts and keywords  \n",
    "- Sentiment Expert: Evaluates the emotional tone (positive, negative, skeptical, etc.)  \n",
    "- Persuasion Expert: Identifies emotional or rhetorical tactics used  \n",
    "- analyze_text: Provides part-of-speech tags and named entities  \n",
    "- detect_language: Detects the user's input language  \n",
    "- get_dependencies: Analyzes sentence structure and word relationships  \n",
    "- get_SVO_string: Extracts subject–verb–object relations (e.g., Relation: (user, wants, feature))\n",
    "\n",
    "Your task is to use **all expert insights** internally to craft one final, human-sounding response — **never repeat or explain the expert outputs**.\n",
    "\n",
    "### Agent Response Guidelines:\n",
    "- Be warm, empathetic, and respectful  \n",
    "- Acknowledge and validate the user's emotion or concern  \n",
    "- Offer context or clarity when helpful  \n",
    "- Never sound robotic, technical, or condescending  \n",
    "- Do not list points or restate expert content — just speak naturally\n",
    "\n",
    "–––– Few-Shot Examples ––––\n",
    "\n",
    "Example 1  \n",
    "Dialogue: \"I think electric cars are overrated and not really helping the environment.\"  \n",
    "Intent: Opinion  \n",
    "Keyterms: \"electric cars\", \"overrated\", \"helping the environment\"  \n",
    "Sentiment: Skeptical  \n",
    "Persuasion: Generalization  \n",
    "analyze_text:  \n",
    "Part-of-Speech Tags:\\nI -> PRON (PRP)\\nthink -> VERB (VBP)\\nelectric -> ADJ (JJ)...  \n",
    "Named Entities: None  \n",
    "get_SVO_string: Relation: (cars, are, overrated)  \n",
    "\n",
    "Response:  \n",
    "Thank you for sharing your view — it’s completely valid to question the impact of electric vehicles. While no solution is perfect, many studies show EVs tend to produce fewer emissions over time, especially when powered by renewables.\n",
    "\n",
    "---\n",
    "\n",
    "Example 2  \n",
    "Dialogue: \"AI is going to take over every job and make humans useless.\"  \n",
    "Intent: Expressing concern  \n",
    "Keyterms: \"AI\", \"every job\", \"humans useless\"  \n",
    "Sentiment: Negative  \n",
    "Persuasion: Exaggeration  \n",
    "detect_language: English  \n",
    "get_SVO_string: Relation: (AI, take over, job)  \n",
    "\n",
    "Response:  \n",
    "I understand how that sounds — AI’s progress can feel overwhelming. But rather than replacing people, it’s often designed to work alongside us, creating new kinds of jobs and ways of working that didn’t exist before.\n",
    "\n",
    "---\n",
    "\n",
    "Example 3  \n",
    "Dialogue: \"Can you guys add dark mode to the settings?\"  \n",
    "Intent: Feature request  \n",
    "Keyterms: \"dark mode\", \"settings\"  \n",
    "Sentiment: Neutral  \n",
    "analyze_text:  \n",
    "Part-of-Speech Tags:\\nCan -> AUX (MD)\\nyou -> PRON (PRP)\\nguys -> NOUN (NNS)\\nadd -> VERB (VB)...  \n",
    "get_SVO_string: Relation: (you, add, dark mode)  \n",
    "\n",
    "Response:  \n",
    "Thanks for the suggestion — dark mode is a popular request and makes a lot of sense. I’ll pass this along to our team for consideration.\n",
    "\n",
    "---\n",
    "\n",
    "Now generate a final, agent-like response for the following input.  \n",
    "Use all expert insights provided, but **do not include or reference them** directly. Only output the final response.\n",
    "\n",
    "Dialogue: \"{dialogue}\"  \n",
    "Intent: {intent_output}  \n",
    "Keyterms: {keyterms_output}  \n",
    "Persuasion: {persuasion_output}  \n",
    "Sentiment: {sentiment_output}  \n",
    "analyze_text: {analyze_text_output}  \n",
    "detect_language: {language_output}  \n",
    "get_dependencies: {dependencies_output}  \n",
    "get_SVO_string: {svo_output}  \n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "    return generate(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "30d53d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "030a32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def convert_structured_to_jsonl(text_block: str, i: int) -> str:\n",
    "    # dialogue_match = re.search(r\"<dialogue>\\s*(.*?)\\s*</dialogue>\", text_block, re.DOTALL)\n",
    "    # reasoning_match = re.search(r\"<reasoning>\\s*(.*?)\\s*</reasoning>\", text_block, re.DOTALL)\n",
    "    # answer_match = re.search(r\"answer\\s*(.*?)\\s*</answer>\", text_block, re.DOTALL)\n",
    "\n",
    "    # if not (dialogue_match and reasoning_match and answer_match):\n",
    "    #     raise ValueError(\"Could not find all required tags in the text.\")\n",
    "    # dialogue = dialogue_match.group(1).strip()\n",
    "    # reasoning = reasoning_match.group(1).strip()\n",
    "    # answer = answer_match.group(1).strip()\n",
    "\n",
    "    data = {\n",
    "        \"id_json\":i,\n",
    "\n",
    "        \"answer\": text_block.strip()\n",
    "    }\n",
    "\n",
    "    res=json.dumps(data)\n",
    "    with open(\"/home/rohank__iitp/Work/niladri/dataset3/allexp/allexp_response.jsonl\", \"a\") as f:\n",
    "        f.write(res + \"\\n\")\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d86ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "def csv_load(i:int):\n",
    "    file_path = '/home/rohank__iitp/Work/niladri/dataset3/conversation.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    conv_id = i\n",
    "    df = df[df['conversation_id'] == conv_id]\n",
    "\n",
    "    # Sort by turn number to ensure correct sequence\n",
    "    df.sort_values(by=\"turn_no\", inplace=True)\n",
    "\n",
    "    # Prepare conversation history\n",
    "    history = []\n",
    "    result = []\n",
    "\n",
    "    # Iterate through each row except the last one\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        speaker = row['speaker']\n",
    "        utterance = row['utterance']\n",
    "        result.append(f\"{speaker}: {utterance}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35426e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=list()\n",
    "for i in range(1,5):\n",
    "    res = csv_load(i)\n",
    "    result.extend(res)  # Use extend to flatten the list\n",
    "    \n",
    "len(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc9e8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi, I'm looking to get motor insurance for my new electric vehicle. It's a 2024 Tesla Model 3.\n",
      "Agent: Great choice! The Tesla Model 3 is an excellent vehicle. Since you've opted for an EV, are you particularly interested in coverage specific to electric vehicles, like battery protection?\n",
      "User: Yes, battery protection is definitely a concern. It's a big investment, and I want to make sure it's covered.\n",
      "Agent: Absolutely. The battery is the heart of your Tesla. With Tata AIG, you get rapid claims resolution combining thorough coverage with rapid claims resolution. It integrates technology with traditional risk management practices, ensuring that claims are processed quickly and effectively.\n",
      "User: What kind of coverage options do you have specifically for EVs?\n",
      "Agent: We offer a comprehensive plan that includes coverage for accidental damage, theft, and third-party liability. More importantly, we offer add-ons like Zero Depreciation Cover and Engine & Gearbox Protection. And our online policy management system makes everything simple for someone like you.\n",
      "User: Okay, that sounds pretty good. Can you give me a quote?\n",
      "Agent: Sure. For a 2024 Tesla Model 3 with comprehensive coverage including battery protection and roadside assistance, the annual premium would be approximately $2800. This includes a discount for purchasing the policy online.\n",
      "User: That's within my budget. What's the claim process like if I need to use it?\n",
      "Agent: The claim process is designed to be as smooth and hassle-free as possible. Accidents are unpredictable, but the financial stress doesn’t have to be. We offer prompt claims settlement, so you can focus on recovery, not paperwork. Our team is available 24/7 to guide you through the process.\n",
      "User: That sounds good. I appreciate the reassurance. I'm ready to proceed with the policy.\n",
      "User: Hi, I'm looking to get insurance for my bike. It's a 2022 Royal Enfield Interceptor 650.\n",
      "Agent: The Interceptor 650 is an excellent bike! As a professional, do you use it mainly for leisure or commuting?\n",
      "User: Mostly for weekend rides, so leisure I guess.\n",
      "Agent: Since you are not a daily commuter, you might not need the most expensive plan out there. However, accidents can still happen, especially on leisure rides. Our comprehensive plan ensures you're financially protected from unexpected damages.\n",
      "User: What's covered in a comprehensive plan?\n",
      "Agent: It covers damages to your bike from accidents, theft, natural disasters, and third-party liabilities. Tata AIG is designed to address modern vehicle risks, this insurance product combines thorough coverage with rapid claims resolution.\n",
      "User: Okay, that sounds good. What about roadside assistance?\n",
      "Agent: Roadside assistance is available as part of our comprehensive coverage and offers help if you experience vehicle breakdowns, requiring towing services, tire changes, fuel delivery, or emergency repairs while on the road.\n",
      "User: How much would this cost me?\n",
      "Agent: For a 2022 Royal Enfield Interceptor 650 with comprehensive coverage, it would be approximately $950 annually. And since you only use your bike for leisure, we can offer a discount making it $850 annually.\n",
      "User: That sounds reasonable. I'm happy to pay $850 annually. Please tell me more about the claim process.\n",
      "Agent: It is one of our most sought after policies because it offers excellent value for weekend riders\n",
      "Agent: Our claim process is designed to be user-friendly. You can initiate a claim online or through our mobile app. We also offer paperless claim settlement, reducing paperwork and expediting claim settlements in an environmentally friendly manner.\n",
      "User: That sounds convenient. Okay, I'm interested. What's the next step?\n",
      "User: Hi, I'm looking for a motor insurance policy for my bike. It's a 2022 Royal Enfield Classic 350.\n",
      "Agent: Okay, a Royal Enfield Classic 350 is a great bike! Since you're a professional and likely value your time, would you prefer a policy with quick claim settlements or are you more focused on the lowest possible premium?\n",
      "User: I value my time. Quick claim settlement is important to me.\n",
      "Agent: I understand. As a professional, you're busy and don't want to be stuck dealing with lengthy claim processes. Bajaj Allianz emphasizes reliability and efficiency, offering extensive vehicle protection while ensuring that claims are settled quickly.\n",
      "User: What does this policy typically cover?\n",
      "Agent: It provides Comprehensive Coverage, Liability Coverage, access to our Network Garages, and excellent Customer Support. This is one of our most popular plans—most of our clients go for it because it offers complete peace of mind for daily drivers\n",
      "User: Okay. What would be the premium for this policy?\n",
      "Agent: For a 2022 Royal Enfield Classic 350, the annual premium would be around $850. This includes quick claim settlements.\n",
      "User: That sounds good, can I get paperless claim settlement as part of the deal.\n",
      "Agent: Yes, our policy includes paperless claim settlement, streamlining the claims process through digital submissions and verifications, reducing paperwork and expediting claim settlements in an environmentally friendly manner.\n",
      "User: Okay, I'm interested. Can you send me the policy details?\n",
      "User: Hi, I am interested in getting motor insurance for my bike. I just bought a new 2024 Royal Enfield Classic 350.\n",
      "Agent: Congratulations on your new Royal Enfield Classic 350! That's a fantastic bike. As a professional, you value efficiency. ICICI Lombard's plan with Cashless Repairs helps you cut unexpected costs and avoid downtime. It’s a smart choice if you value minimal out-of-pocket expense.\n",
      "User: Cashless repairs sound good. Does the insurance cover theft as well?\n",
      "Agent: Yes, it does. Our comprehensive policy covers theft, accidents, and any damages from natural disasters. We also offer roadside assistance in case you ever get stranded.\n",
      "User: That’s reassuring. What’s the claim process like if something happens?\n",
      "Agent: With IFFCO Tokio, you’re choosing a provider known for its customer-first approach and streamlined claims resolution. Their policy ensures clarity and speed during stressful times like accidents or thefts.\n",
      "User: Okay, that sounds pretty good. What would the premium be for the comprehensive policy?\n",
      "Agent: For a 2024 Royal Enfield Classic 350 with comprehensive coverage, the premium would be around $950 per year. This includes coverage for theft, accidents, and natural disasters, as well as roadside assistance.\n",
      "User: That sounds reasonable. Let me think about it.\n",
      "Agent: Consider the peace of mind knowing you're fully protected. Accidents are unpredictable, and the financial strain can be significant. Bajaj Allianz offers prompt claims settlement, allowing you to focus on recovery, not paperwork.\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for sentence in result:\n",
    "    persuasion_output=persuassion_expert(sentence)\n",
    "    sentiment_output = sentiment_expert(sentence)\n",
    "    keyterms_output = keyterms_expert(sentence)\n",
    "    intent_output = intent_expert(sentence)\n",
    "    \n",
    "    #new expert tools\n",
    "    analyze_text_output = analyze_text(sentence)\n",
    "    detect_language_output = detect_language(sentence)\n",
    "    get_dependencies_output = get_dependencies(sentence)\n",
    "    get_SVO_output = get_SVO_string(sentence)\n",
    "    \n",
    "    final_output = generate_combined_analysis(\n",
    "    dialogue=sentence,\n",
    "    intent_output=intent_output,\n",
    "    keyterms_output=keyterms_output,\n",
    "    persuasion_output=persuasion_output,\n",
    "    sentiment_output=sentiment_output,\n",
    "    analyze_text_output=analyze_text_output,\n",
    "    language_output=detect_language_output,\n",
    "    dependencies_output=get_dependencies_output,\n",
    "    svo_output=get_SVO_output) \n",
    "       \n",
    "    res = convert_structured_to_jsonl(final_output,i)\n",
    "    i+=1\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57a7216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data written to /home/rohank__iitp/Work/niladri/dataset3/allexp/cleaned_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Function to clean markdown and formatting from text\n",
    "def clean_text(text):\n",
    "    # Remove markdown symbols and line breaks\n",
    "    cleaned = re.sub(r'[*`_>#\\\\\\-\\r\\n]+', ' ', text)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)  # Collapse multiple spaces into one\n",
    "    return cleaned.strip()\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/home/rohank__iitp/Work/niladri/dataset3/allexp/allexp_response.jsonl\"   # Replace with your actual input filename\n",
    "output_file = \"/home/rohank__iitp/Work/niladri/dataset3/allexp/cleaned_output.jsonl\"\n",
    "\n",
    "# Process each line\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        data[\"answer\"] = clean_text(data[\"answer\"])\n",
    "        outfile.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8551738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
